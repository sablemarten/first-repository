{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E11 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "* unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [' There must be some kind of way outta here', 'Said the joker to the thief', \"There's too much confusion\", \"I can't get no relief Business men, they drink my wine\", 'Plowman dig my earth', 'None were level on the mind', 'Nobody up at his word', 'Hey, hey No reason to get excited', 'The thief he kindly spoke', 'There are many here among us', 'Who feel that life is but a joke', \"But, uh, but you and I, we've been through that\", 'And this is not our fate', \"So let us stop talkin' falsely now\", \"The hour's getting late, hey All along the watchtower\"]\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel//lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 정제  \n",
    "* 문장을 토큰화했을 경우 토큰 개수 15개 넘을 시 잘라냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> there must be some kind of way outta here <end>',\n",
       " '<start> said the joker to the thief <end>',\n",
       " '<start> there s too much confusion <end>',\n",
       " '<start> i can t get no relief business men , they drink my wine <end>',\n",
       " '<start> plowman dig my earth <end>',\n",
       " '<start> none were level on the mind <end>',\n",
       " '<start> nobody up at his word <end>',\n",
       " '<start> hey , hey no reason to get excited <end>',\n",
       " '<start> the thief he kindly spoke <end>',\n",
       " '<start> there are many here among us <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  65 280 ...   0   0   0]\n",
      " [  2 107   6 ...   0   0   0]\n",
      " [  2  65  16 ...   0   0   0]\n",
      " ...\n",
      " [  2  39  39 ...   0   0   0]\n",
      " [  2   5  22 ...   0   0   0]\n",
      " [  2  39  39 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7fcaa3de9410>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15) #토큰 개수 15개\n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  65 280  27  99 528  19  85 778  93   3   0   0   0]\n",
      "[ 65 280  27  99 528  19  85 778  93   3   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1] # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:] # tensor에서 <START>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 14)\n",
      "Target Train: (140599, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset\n",
    "#데이터 전처리 끝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. 인공지능 만들기\n",
    "* 모델의 Embedding Size, Hidden Size 조절하여 10 Epoch안에 val_loss 값을 2.2수준으로 줄일 수 있는 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 4.4241818e-04, -3.9798433e-05, -2.5222445e-04, ...,\n",
       "          1.4962131e-04,  7.8246550e-05,  2.0173859e-05],\n",
       "        [ 5.3515943e-04, -3.1960994e-05, -4.5926372e-05, ...,\n",
       "          3.9831811e-04, -1.3332973e-04,  9.9574965e-05],\n",
       "        [ 5.8413588e-04, -1.6642768e-04,  7.9467958e-05, ...,\n",
       "          6.0163817e-04, -6.1994983e-04,  5.8025360e-04],\n",
       "        ...,\n",
       "        [ 5.0016359e-04,  5.3034333e-04, -1.5058708e-03, ...,\n",
       "         -5.6587893e-04,  2.7353494e-04,  5.4724119e-04],\n",
       "        [ 4.2798588e-04,  9.6645148e-04, -2.0921095e-03, ...,\n",
       "         -7.6897931e-04,  4.1347416e-04,  2.5786928e-04],\n",
       "        [ 3.4619489e-04,  1.3182849e-03, -2.6700122e-03, ...,\n",
       "         -1.0534760e-03,  4.7058676e-04,  7.5982854e-05]],\n",
       "\n",
       "       [[ 4.4241818e-04, -3.9798433e-05, -2.5222445e-04, ...,\n",
       "          1.4962131e-04,  7.8246550e-05,  2.0173859e-05],\n",
       "        [ 9.6222089e-04,  1.1019529e-06, -3.5704474e-04, ...,\n",
       "          1.5334945e-04, -1.8077984e-04,  2.0789837e-05],\n",
       "        [ 1.4741159e-03, -4.7205380e-04, -5.8007590e-04, ...,\n",
       "          5.8039197e-05, -4.8181359e-05,  1.2566728e-05],\n",
       "        ...,\n",
       "        [ 2.4618130e-04, -5.0168764e-04, -1.9586931e-03, ...,\n",
       "         -6.0258963e-04,  2.6833233e-05,  1.4844378e-04],\n",
       "        [ 4.0471627e-05,  9.9841483e-05, -2.4633645e-03, ...,\n",
       "         -8.6329423e-04,  1.7044909e-04,  1.7940838e-04],\n",
       "        [-1.4291957e-04,  6.2666967e-04, -2.9742667e-03, ...,\n",
       "         -1.2098432e-03,  2.4950289e-04,  2.3812518e-04]],\n",
       "\n",
       "       [[-8.9711291e-05, -1.0924358e-04,  2.0849175e-04, ...,\n",
       "          1.6961123e-04, -3.7582088e-04, -2.6166893e-04],\n",
       "        [-2.8769858e-04, -5.1410130e-04,  4.2430623e-04, ...,\n",
       "          9.0866626e-05, -7.7095610e-04, -1.6575748e-04],\n",
       "        [-3.0142514e-04, -8.7958545e-04,  5.1701652e-05, ...,\n",
       "         -1.6981574e-04, -1.0660671e-03, -1.0483617e-04],\n",
       "        ...,\n",
       "        [-1.0921620e-03, -2.1280576e-03, -6.2607799e-04, ...,\n",
       "          3.6864285e-04, -5.1988463e-04, -8.4636422e-06],\n",
       "        [-8.7343570e-04, -2.1165663e-03, -7.9865160e-04, ...,\n",
       "          5.3190434e-04, -5.2786060e-04, -2.0006188e-04],\n",
       "        [-9.1895211e-04, -2.1564248e-03, -1.3540414e-03, ...,\n",
       "          2.0535514e-04, -4.4842568e-04, -1.6795009e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 4.4241818e-04, -3.9798433e-05, -2.5222445e-04, ...,\n",
       "          1.4962131e-04,  7.8246550e-05,  2.0173859e-05],\n",
       "        [ 1.0018889e-03, -2.1959738e-04, -1.0178130e-04, ...,\n",
       "         -1.1620558e-04,  1.8867748e-04,  8.0132784e-05],\n",
       "        [ 8.8545866e-04, -1.5962753e-04, -2.3260526e-04, ...,\n",
       "          3.1144943e-04,  1.3745298e-04, -2.0192222e-04],\n",
       "        ...,\n",
       "        [ 3.1084687e-04, -1.9342036e-03, -1.1291500e-03, ...,\n",
       "         -1.0162522e-03,  9.2776865e-04, -1.2992736e-03],\n",
       "        [ 3.1524297e-04, -1.2864914e-03, -1.8067432e-03, ...,\n",
       "         -1.4244432e-03,  1.0849445e-03, -1.3727390e-03],\n",
       "        [ 2.2529045e-04, -6.1033515e-04, -2.5358235e-03, ...,\n",
       "         -1.6869777e-03,  1.1193983e-03, -1.3921058e-03]],\n",
       "\n",
       "       [[ 4.4241818e-04, -3.9798433e-05, -2.5222445e-04, ...,\n",
       "          1.4962131e-04,  7.8246550e-05,  2.0173859e-05],\n",
       "        [ 7.2919804e-04, -1.6279788e-04, -1.8261782e-04, ...,\n",
       "          1.6201362e-04, -1.6404545e-05,  4.1497671e-05],\n",
       "        [ 7.8618736e-04,  1.6200669e-04, -2.7120931e-04, ...,\n",
       "          3.7409164e-04,  2.1514026e-04,  3.1551687e-04],\n",
       "        ...,\n",
       "        [-1.9628888e-03,  2.9320666e-03, -3.1127168e-03, ...,\n",
       "         -5.8082386e-04, -1.7390288e-05, -9.6886954e-04],\n",
       "        [-1.9298792e-03,  3.0691449e-03, -3.6727262e-03, ...,\n",
       "         -9.9439814e-04, -5.6754812e-05, -8.3309691e-04],\n",
       "        [-1.8648454e-03,  3.1193118e-03, -4.1927453e-03, ...,\n",
       "         -1.4830107e-03, -9.8897573e-05, -6.4768118e-04]],\n",
       "\n",
       "       [[ 4.4241818e-04, -3.9798433e-05, -2.5222445e-04, ...,\n",
       "          1.4962131e-04,  7.8246550e-05,  2.0173859e-05],\n",
       "        [ 1.0018889e-03, -2.1959738e-04, -1.0178130e-04, ...,\n",
       "         -1.1620558e-04,  1.8867748e-04,  8.0132784e-05],\n",
       "        [ 1.2426698e-03, -3.4643681e-04,  1.3095267e-04, ...,\n",
       "         -1.8000386e-04,  5.4105540e-04, -7.1087758e-05],\n",
       "        ...,\n",
       "        [-1.0374350e-03, -5.9186551e-04, -1.7080158e-03, ...,\n",
       "         -3.1225255e-04, -7.5071148e-04, -4.1477297e-06],\n",
       "        [-1.2980439e-03, -7.0783315e-04, -2.3676136e-03, ...,\n",
       "         -6.9950911e-04, -5.7670817e-04, -3.3126242e-04],\n",
       "        [-1.1722246e-03, -4.3951411e-04, -2.6909765e-03, ...,\n",
       "         -1.2703781e-03, -4.8197756e-04, -4.1031433e-04]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  6144512   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  6295552   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 33,133,793\n",
      "Trainable params: 33,133,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "549/549 [==============================] - 89s 163ms/step - loss: 3.6467\n",
      "Epoch 2/10\n",
      "549/549 [==============================] - 91s 166ms/step - loss: 3.1404\n",
      "Epoch 3/10\n",
      "549/549 [==============================] - 92s 168ms/step - loss: 2.9527\n",
      "Epoch 4/10\n",
      "549/549 [==============================] - 91s 166ms/step - loss: 2.8074\n",
      "Epoch 5/10\n",
      "549/549 [==============================] - 94s 172ms/step - loss: 2.6784\n",
      "Epoch 6/10\n",
      "549/549 [==============================] - 92s 168ms/step - loss: 2.5583\n",
      "Epoch 7/10\n",
      "549/549 [==============================] - 90s 163ms/step - loss: 2.4451\n",
      "Epoch 8/10\n",
      "549/549 [==============================] - 90s 163ms/step - loss: 2.3387\n",
      "Epoch 9/10\n",
      "549/549 [==============================] - 83s 152ms/step - loss: 2.2361\n",
      "Epoch 10/10\n",
      "549/549 [==============================] - 78s 142ms/step - loss: 2.1375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcaa23f17d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <END>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love it when you call me big poppa <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아무래도 maxlen을 어떻게 처리할지가 관건이었다.\n",
    "\n",
    "tokenize함수 tensor에 maxlen=15를 넣으면 15가 넘어갈 경우 그 문장의 뒤를 싹둑 잘라서 문장의 완성형 형태가 아니게 되므로 학습에 혼란을 주는 경우가 발생한다는 정보를 얻었다. 우선 Step 3에서 제안된 방식이 잘라내는 것이므로 여기서는 다른 선택을 하지 않았지만 다른 조원이 한 것처럼 15를 넘어가는 문장을 빼버리는 과정을 거치는 것도 나쁘지 않다고 생각했다.\n",
    "하지만 그렇게 하기 위해서는 그 문장들의 개수가 몇개인지 확인하는 과정을 거치고 그 중요도를 파악한 후에 해야 한다는 생각이 들었고, 안그래도 진행이 늦은 입장에서 그것을 빨리 처리할 자신이 없었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
